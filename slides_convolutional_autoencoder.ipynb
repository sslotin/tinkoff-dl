{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>Автоэнкодеры</h1></center>\n",
    "\n",
    "![ae](https://habrastorage.org/web/cf6/228/613/cf6228613fdc4f8fb819cbd41bb677eb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Автоэнкодеры** — это сети, которые учатся восстанавливать свои же входные данные.\n",
    "\n",
    "Казалось бы, очень легко выучить функцию $f(x) = x$, но в автоэнкодеры устроены так, что внутри них вся информация в какой-то момент проходит через скрытый слой небольшой размерности. Поэтому автоэнкодеры, как правило, не имеют возможности идеально точно скопировать свой вход на выходе.\n",
    "\n",
    "Мы заставляем сеть выучить очень сжатое и информативное представление данных, что потом можно будет использовать для разных интересных вещей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Например, для визуализации.\n",
    "\n",
    "![vis](https://i.stack.imgur.com/2gSs1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PCA\n",
    "\n",
    "**PCA** (англ. Principal Component Analysis) — это частный случай автоэнкодера.\n",
    "\n",
    "Даны точки в $n$ мерном пространстве. Нужно подобрать такую $k$-мерную гиперплоскость ($k < n$), что сумма квадратов расстояний исходных точек до их проекций на неё минимально."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "encoder = nn.Linear(784, 2)\n",
    "decoder = nn.Linear(2, 784)\n",
    "\n",
    "criterion = MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "(На самом деле, это не совсем то же, но в детали лезть не будем.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "PCA используют, потому что у него есть аналитическое решение — через сингулярное разложение (SVD). Не надо это гуглить, там пока слишком много математики для вас.\n",
    "\n",
    "Большинство же автоэнкодеров обучаются, как обычные нейросети (долго и мучительно, а также оверфитятся)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# У Linear слишком много параметров\n",
    "\n",
    "Вообще, задача «нейроинженеров» — думать, как выглядело бы решение этой задачи на уровне программы с неизвестными параметрами, и подбирать соответствующую архитектуру.\n",
    "\n",
    "Эксперименты с дропаутом же показывают, что в Linear примерно 99% весов на самом деле можно выкинуть. Логично, что в оптимальной архитектуре не должно быть бесполезных весов — лишние параметры всегда ведут к переобучению."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "В случае с картинками, решение в том, чтобы использовать информацию о том, как расположены пиксели относительно друг друга, чтобы создать слой, который смотрит на более релевантные фичи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>Свёртки</h1></center>\n",
    "\n",
    "![CNN](https://habrastorage.org/files/c36/bc5/b99/c36bc5b99dc14342b156fa742b285418.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Как хранятся картинки\n",
    "\n",
    "Когда говорят «изображение», представляйте не прямоугольник, а параллелепипед, высотой которого будет размер каналов.\n",
    "\n",
    "![conv1](images/conv1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Как считается\n",
    "\n",
    "0. Введем такую функцию, как **ядро** (англ. **kernel**) — она считает скалярное произведение вектора-входа с вектором-параметром.\n",
    "1. Разобьем исходный паралеллелепипед на сколько-то одинаковых параллелепипедов. Они могут пересекаться.\n",
    "2. Каждый из них «разгладим» в вектор.\n",
    "3. К кажому такому вектору и применим по очереди каждый кернел (их обычно берут много разных).\n",
    "4. Положим то, что получилось, в новый параллелепипед.\n",
    "5. Посчитаем для кажой ячейки какую-нибудь нелинейность. Обычно это ReLU из-за вычислительных причин.\n",
    "\n",
    "Вдохновлено зрительной корой нашего мозга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Пулинг\n",
    "\n",
    "Pooling нужен для понижения размерности.\n",
    "\n",
    "Разрежем каждый слой на квадратики и посчитаем на них максимум. Обычно квадратики берут 2x2, тогда количество фичей уменьшится в четыре раза.\n",
    "\n",
    "Никаких параллелепипедов, кернелов и обучаемых параметров. Здесь всё просто.\n",
    "\n",
    "Интуиция: на каждом фильтре детектится какой-то объект. Логично, что уверенность присутствия какого-то объекта на какой-то области — это максимум из уверенностей его присутствия на четырех областях поменьше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "У пулинга есть фундаментальная проблема — будучи максимумом, он теряет информацию.\n",
    "\n",
    "Джеффри Хинтон, отец-основатель DL, много думал об этом и вооще о том, как работает зрение на уровне психологии, и придумал в 2017-м *капсульные сети*, но о них как-нибудь в другой раз."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
